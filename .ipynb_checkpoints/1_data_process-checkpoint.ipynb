{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "# import statsmodels.api as sm\n",
    "from xgboost import XGBClassifier\n",
    "#import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "# from shapely import wkt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lists# Long-ass lists of vars etc. (keep in same folder)\n",
    "import utils# Define util functions\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATH = \"/Users/david.duong/dev/lending-club/\"\n",
    "FNAME= \"accepted_2007_2018.csv\"\n",
    "QAFNAME = \"qavals.pkl\"\n",
    "\n",
    "DEFAULT_CATS = ['Default', 'Charged Off', 'Does not meet the credit policy. Status:Charged Off']\n",
    "FULLY_PAID_CATS = ['Does not meet the credit policy. Status:Fully Paid','Fully Paid']\n",
    "\n",
    "FILTER_TERMS = [\" 36 months\"]# Remove the 60 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260701, 151)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv(FPATH + FNAME)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1a. Round-1 Filters (NA, Terms, Determinant labels etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop the NA value...\n",
      "(2260667, 151)\n",
      "(1609753, 151)\n",
      "(1023205, 151)\n"
     ]
    }
   ],
   "source": [
    "def apply_filters1(dat):\n",
    "    # Drop NAs:\n",
    "    dat = dat.loc[~dat.loan_amnt.isnull()]\n",
    "    print ('Drop the NA value...')\n",
    "    dat = dat.dropna(subset = ['zip_code'])\n",
    "    print(dat.shape)\n",
    "\n",
    "    # Keep only our filters:\n",
    "    dat = dat[dat['term'].isin(FILTER_TERMS)]\n",
    "    print(dat.shape)\n",
    "\n",
    "    # Keep only determinant labels (e.g. clear defaults or clear paid)\n",
    "    dat = dat[dat['loan_status'].isin(DEFAULT_CATS + FULLY_PAID_CATS)]\n",
    "    print(dat.shape)\n",
    "    return dat\n",
    "\n",
    "dat = apply_filters1(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1b. Create labels & remove non-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default loans among all samples: 163926 at 16.02 percent\n",
      "Number of paid loans: 859279\n",
      "(1023205, 153)\n"
     ]
    }
   ],
   "source": [
    "# Check default loans:\n",
    "def create_labels(dat):\n",
    "    dat['Default_flag'] = dat['loan_status'].apply(lambda x: 1 if x in DEFAULT_CATS else 0)\n",
    "\n",
    "    print('Default loans among all samples: {} at {:.2f} percent'.format(\n",
    "        sum(dat['Default_flag']), \n",
    "        sum(dat['Default_flag'])/dat.shape[0]*100))\n",
    "\n",
    "    # Get paid flag\n",
    "    dat['Paid_flag'] = dat['loan_status'].apply(lambda x: 1 if x in FULLY_PAID_CATS else 0)\n",
    "    print ('Number of paid loans:', sum(dat.Paid_flag))\n",
    "    ###\n",
    "    return dat\n",
    "    \n",
    "dat = create_labels(dat)\n",
    "\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs with DATEs & SAMPLEs: \n",
    "\"\"\"\n",
    "1. Review and code the labels like I should\n",
    "2. Think about what samples I would use for training, and for validation, based on bad rates\n",
    "2b. Write function to flexibly pick samples for training and for validation\n",
    "\n",
    "3. Review my data to see where I have different distribution in loan durations etc.\n",
    "(to inform my picking of samples)\n",
    "\n",
    "7. (Later but soon) Check distribution of all variables for all the different snapshots\n",
    "7b. Also note that the changing distribution can lead us to use fewer snapshots to train etc.\n",
    "\n",
    "9. Do a ...\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1c. Issue dates & Months (for (i) data review (ii) keep to split later)\n",
    "TODO: Use Years/ Months to redo my sampling (use smaller data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convenient issue year-month vars (for sample splitting etc.)\n",
    "def create_yyyymm(y,m):\n",
    "    if m < 10:\n",
    "        return (str(y) + \"0\" + str(m))\n",
    "    else:\n",
    "        return (str(y) + str(m))\n",
    "\n",
    "def create_timing_cols(dat):\n",
    "    dat['issue_d'] = pd.to_datetime(dat['issue_d'], format = '%b-%Y')\n",
    "    dat['issue_year'] = dat['issue_d'].apply(lambda d: d.year)\n",
    "    dat['issue_ym'] = dat['issue_d'].apply(lambda d: create_yyyymm(d.year, d.month))\n",
    "    return dat\n",
    "\n",
    "dat = create_timing_cols(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Description: \\n200706 and 200707: Pre-recession\\n200708-200807: Recession 20% - 30%\\n200808-200901: ~ 15%\\n200902+: Not bad\\n201603 - 201709: ~ 20% again wtf\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby('issue_ym')['Default_flag'].mean().sort_index().head()\n",
    "\"\"\"Description: \n",
    "200706 and 200707: Pre-recession\n",
    "200708-200807: Recession 20% - 30%\n",
    "200808-200901: ~ 15%\n",
    "200902+: Not bad\n",
    "201603 - 201709: ~ 20% again wtf\n",
    "\"\"\"\n",
    "# Comment: Maybe the distribution of loan types has changed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp\n",
    "dat[(dat['issue_ym'] == '201410') & (dat['Default_flag'] == 1)].groupby('last_pymnt_d')['Default_flag'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA1. Output the check list for the vars' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "def get_cont_distribution_dict(dat, varlist=[], filter_ys = None):\n",
    "    \"\"\"filter is None or a list of years\"\"\"\n",
    "    # Temp: Check distribution of vars\n",
    "    if filter_ys != None:# If there's a filter     \n",
    "        dat = dat[dat['issue_year'].isin(filter_ys)]\n",
    "    \n",
    "    tempdict = {}    \n",
    "    for col in varlist:\n",
    "        tempdict[col] = {}\n",
    "    \n",
    "    for col in varlist:\n",
    "        tempdict[col]['min'] = np.nanmin(dat[col].values)\n",
    "        tempdict[col]['max'] = np.nanmax(dat[col].values)\n",
    "        tempdict[col]['95th'] = np.nanpercentile(dat[col].values, 95)\n",
    "        # TODO: Add missingness % as well\n",
    "    print(len(tempdict))\n",
    "    return tempdict\n",
    "\n",
    "#get_cont_distribution_dict = utils.get_cont_distribution_dict# TODO: Uncomment this later\n",
    "\n",
    "qavals_dict = {'continuous':{}, 'cat':{}}\n",
    "qavals_dict['continuous'] = get_cont_distribution_dict(dat, varlist = lists.continuous_cols_4_model, filter_ys = [2015])\n",
    "\n",
    "# Now output\n",
    "pickle.dump(qavals_dict, open(FPATH + QAFNAME, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48625"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.bc_open_to_buy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... Get data pre-feature-engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: (1348098, 109)\n"
     ]
    }
   ],
   "source": [
    "# FULL DATA\n",
    "training_data = dat[lists.cols_to_use_round1 + ['Default_flag']]\n",
    "\n",
    "print('Full data:', training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 3 digit zipcode with high risks (for dummy_creation later)\n",
    "ToDo 01/09/20: Might group some zips together to make more than one var\n",
    "Or might create one var only that has levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['516', '938', '682', '568', '524', '513', '205', '889', '643', '692', '502', '345', '901', '555', '528', '522', '742', '663', '709', '504', '503', '203', '738', '736', '835', '415', '343', '204', '569', '353', '621', '746', '833', '739', '683', '438', '690']\n"
     ]
    }
   ],
   "source": [
    "def find_highrisk_zips(dat):\n",
    "    dat['zip3digit'] = dat.zip_code.apply(lambda z: z[:3])\n",
    "\n",
    "    # convert zip3digit variable to string\n",
    "    dat['zip3digit'] = dat.zip3digit.astype(str)\n",
    "\n",
    "    # Got top zipcodes that have largest average default rate\n",
    "    zipcode_default = dat.groupby('zip3digit')['Default_flag'].sum()\n",
    "    zipcode_all = dat.groupby('zip3digit').size()\n",
    "    zipcode_default_ratio = zipcode_default/zipcode_all\n",
    "    zipcode_default_ratio = zipcode_default_ratio.sort_values(ascending=False)\n",
    "\n",
    "    # Get numer of zipcode to collect using a default threshold\n",
    "    topn = (zipcode_default_ratio > 0.3).sum()\n",
    "\n",
    "    # Get the list of high risk zipcode\n",
    "    highr_zips = zipcode_default_ratio[:topn].index.tolist()\n",
    "    return highr_zips\n",
    "\n",
    "highrisk_zips = find_highrisk_zips(dat)\n",
    "print(highrisk_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.isnan(dat.mths_since_last_major_derog.values[1])\n",
    "#dat.mths_since_last_major_derog.values[1] == np.NaN\n",
    "np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some special variables to notice: replace NA by 0 would be wrong `mths_since_last_major_derog`, `mths_since_recent_bc_dlq`, `mths_since_recent_revol_delinq` ==> have dummy variables to mark those NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_pandas(df, cat_colname):\n",
    "    '''\n",
    "    Create one-hot-encode columns for categorical variables\n",
    "   \n",
    "    Input: a dataframe with name of column that has categorical data\n",
    "    Output: a dataframe with number of columns equivalent to number of categories\n",
    "    '''\n",
    "    df[cat_colname] = pd.Categorical(df[cat_colname])\n",
    "    dfDummies = pd.get_dummies(df[cat_colname], prefix = cat_colname)\n",
    "   \n",
    "    return dfDummies\n",
    "\n",
    "def process_train_data(df, highRiskZip):    \n",
    "    \n",
    "    # Get 3 digit zipcode\n",
    "    df['zip3digit'] = df.zip_code.apply(lambda z: z[:3])\n",
    "    \n",
    "    # Convert zip3digit to str:\n",
    "    df['zip3digit'] = df.zip3digit.astype(str)\n",
    "    \n",
    "    # Create a column to flag people in high risk location\n",
    "    df['highRiskZip'] = df['zip3digit'].apply(lambda x: 1 if x in highRiskZip else 0)\n",
    "    \n",
    "    # Create dummy variable to flag month_since_$ variables\n",
    "    age_vars = ['mths_since_last_major_derog','mths_since_recent_bc_dlq','mths_since_recent_revol_delinq']\n",
    "\n",
    "    dummy_vars = ['major_derog_NA','bc_dlq_NA','revol_delinq_NA']\n",
    "\n",
    "    for a,d in zip(age_vars, dummy_vars):\n",
    "        df[d] = df[a].isnull().astype(int)\n",
    "    \n",
    "    # Check category variables\n",
    "    all_cols = df.columns.tolist()\n",
    "    cat_vars = []\n",
    "    for c in all_cols:\n",
    "        try:\n",
    "            if df[c].dtype == 'O':\n",
    "                cat_vars.append(c)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('Categorical Variables:',cat_vars)\n",
    "\n",
    "    # Convert all cat columns to str\n",
    "    for c in cat_vars:\n",
    "        df[c] = df[c].astype(str)    \n",
    "        \n",
    "    \n",
    "    # Fill 0 to NA for other columns\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    # One-hot-encode categorical variables\n",
    "    dfCats = pd.DataFrame()\n",
    "\n",
    "    for col in cat_vars:\n",
    "        dfDummies = one_hot_encode_pandas(df, col)\n",
    "\n",
    "        assert dfDummies.shape[0] == df.shape[0]\n",
    "\n",
    "        dfCats = pd.concat([dfCats, dfDummies], axis=1)\n",
    "\n",
    "\n",
    "    df = pd.concat([df, dfCats], axis = 1)\n",
    "    #df = df.drop(cat_vars, axis = 1) #Drop original categorical columns\n",
    "\n",
    "    #df = df.drop(cat_var_to_drop, axis = 1)\n",
    "    \n",
    "    # Rename one column so XGBoost can fit the model\n",
    "    df.rename(columns = {'emp_length_< 1 year': 'emp_length_Less 1 year'}, inplace = True)\n",
    "    \n",
    "    # Drop grade E and F cause they are no longer offered.\n",
    "    #df.drop(['grade_E','grade_F'], axis = 1, inplace=True)\n",
    "    \n",
    "    # Keep only home_ownership RENT, MORTGAGE, OWN\n",
    "    \"\"\"# DD: Remove because I will keep using a master list\n",
    "    home_ownership_toKeep = ['home_ownership_RENT','home_ownership_MORTGAGE','home_ownership_OWN']\n",
    "    home_ownership_cols = [c for c in df.columns if 'home_ownership' in c]\n",
    "    home_ownership_todrop = [c for c in home_ownership_cols if c not in home_ownership_toKeep]\n",
    "    if len(home_ownership_todrop) >0:\n",
    "        df.drop(home_ownership_todrop, axis = 1, inplace=True)\n",
    "    \"\"\"\n",
    "    df = df[lists.cols_to_use_post_feature_eng]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineer_numeric_vars(df, numeric_vars):\n",
    "    \"\"\"This takes numeric vars and process them (fill missings) then outputs that list with features only\"\"\"\n",
    "    return df[['id'] + numeric_vars].fillna(0)# Note btw that id doesn't have missing so it won't be filled w any 0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1023205, 76)\n"
     ]
    }
   ],
   "source": [
    "numeric_post_fe_df = feature_engineer_numeric_vars(dat, lists.continuous_cols_4_model)\n",
    "print(numeric_post_fe_df.shape)\n",
    "\n",
    "#training_data2 = process_train_data(training_data, highrisk_zips)\n",
    "\n",
    "#print(training_data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file\n",
    "#training_data2.to_csv('cleaned_default_data_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
