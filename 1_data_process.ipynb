{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lists of vars and printing out some list sizes...\n",
      "75\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "# import statsmodels.api as sm\n",
    "from xgboost import XGBClassifier\n",
    "#import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "# from shapely import wkt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Not importing lists bc run lists script below\n",
    "#import lists# Long-ass lists of vars etc. (keep in same folder)\n",
    "import utils# Define util functions\n",
    "\n",
    "%matplotlib inline\n",
    "%run lists.py\n",
    "\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATH = \"/Users/david.duong/dev/lending-club/\"\n",
    "FNAME= \"accepted_2007_2018.csv\"\n",
    "OFNAME = \"post_fe_dat.csv\"\n",
    "\n",
    "QAFNAME = \"qavals.pkl\"\n",
    "\n",
    "DEFAULT_CATS = ['Default', 'Charged Off', 'Does not meet the credit policy. Status:Charged Off']\n",
    "FULLY_PAID_CATS = ['Does not meet the credit policy. Status:Fully Paid','Fully Paid']\n",
    "\n",
    "FILTER_TERMS = [\" 36 months\"]# Remove the 60 months\n",
    "\n",
    "# List of cat vars to use for feature engineering\n",
    "CAT_VARS = ['term', 'grade', 'emp_length', 'home_ownership', \n",
    "            'verification_status', 'purpose', 'initial_list_status', \n",
    "            'application_type', 'disbursement_method']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260701, 151)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv(FPATH + FNAME)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1a. Round-1 Filters (NA, Terms, Determinant labels etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop the NA value...\n",
      "(1023205, 151)\n",
      "(1023205, 151)\n",
      "(1023205, 151)\n"
     ]
    }
   ],
   "source": [
    "def apply_filters1(dat):\n",
    "    # Drop NAs:\n",
    "    dat = dat.loc[~dat.loan_amnt.isnull()]\n",
    "    print ('Drop the NA value...')\n",
    "    dat = dat.dropna(subset = ['zip_code'])\n",
    "    print(dat.shape)\n",
    "\n",
    "    # Keep only our filters:\n",
    "    dat = dat[dat['term'].isin(FILTER_TERMS)]\n",
    "    print(dat.shape)\n",
    "\n",
    "    # Keep only determinant labels (e.g. clear defaults or clear paid)\n",
    "    dat = dat[dat['loan_status'].isin(DEFAULT_CATS + FULLY_PAID_CATS)]\n",
    "    print(dat.shape)\n",
    "    return dat\n",
    "\n",
    "dat = apply_filters1(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1b. Create labels & remove ambiguous labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default loans among all samples: 163926 at 16.02 percent\n",
      "Number of paid loans: 859279\n",
      "(1023205, 153)\n"
     ]
    }
   ],
   "source": [
    "# Check default loans:\n",
    "def create_labels(dat):\n",
    "    dat['Default_flag'] = dat['loan_status'].apply(lambda x: 1 if x in DEFAULT_CATS else 0)\n",
    "\n",
    "    print('Default loans among all samples: {} at {:.2f} percent'.format(\n",
    "        sum(dat['Default_flag']), \n",
    "        sum(dat['Default_flag'])/dat.shape[0]*100))\n",
    "\n",
    "    # Get paid flag\n",
    "    dat['Paid_flag'] = dat['loan_status'].apply(lambda x: 1 if x in FULLY_PAID_CATS else 0)\n",
    "    print ('Number of paid loans:', sum(dat.Paid_flag))\n",
    "    ###\n",
    "    return dat\n",
    "    \n",
    "dat = create_labels(dat)\n",
    "\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Review and code the labels like I should (e.g. should paricular cats go into 1,\\nand others go into 0?)\\n\\nx. Later: Consider whether to include initial_list_status = w\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN TODOs\n",
    "\"\"\"\n",
    "\n",
    "1. Review and code the labels like I should (e.g. should paricular cats go into 1,\n",
    "and others go into 0?)\n",
    "\n",
    "x. Later: Consider whether to include initial_list_status = w\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP1c. Issue dates & Months (for (i) data review (ii) keep to split later)\n",
    "TODO: Use Years/ Months to redo my sampling (use smaller data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convenient issue year-month vars (for sample splitting etc.)\n",
    "def create_yyyymm_util(y,m):\n",
    "    if m < 10:\n",
    "        return (str(y) + \"0\" + str(m))\n",
    "    else:\n",
    "        return (str(y) + str(m))\n",
    "\n",
    "def create_timing_cols(dat):\n",
    "    dat['issue_d'] = pd.to_datetime(dat['issue_d'], format = '%b-%Y')\n",
    "    dat['issue_year'] = dat['issue_d'].apply(lambda d: d.year)\n",
    "    dat['issue_ym'] = dat['issue_d'].apply(lambda d: create_yyyymm_util(d.year, d.month))\n",
    "    return dat\n",
    "\n",
    "dat = create_timing_cols(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Description: \\n200706 and 200707: Pre-recession\\n200708-200807: Recession 20% - 30%\\n200808-200901: ~ 15%\\n200902+: Not bad\\n201603 - 201709: ~ 20% again wtf\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dat.groupby('issue_ym')['Default_flag'].mean().sort_index().head()\n",
    "\"\"\"Description: \n",
    "200706 and 200707: Pre-recession\n",
    "200708-200807: Recession 20% - 30%\n",
    "200808-200901: ~ 15%\n",
    "200902+: Not bad\n",
    "201603 - 201709: ~ 20% again wtf\n",
    "\"\"\"\n",
    "# Comment: Maybe the distribution of loan types has changed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA1. Output the check list for the vars' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "def get_cont_distribution_dict(dat, varlist=[], filter_ys = None):\n",
    "    \"\"\"filter is None or a list of years\"\"\"\n",
    "    # Temp: Check distribution of vars\n",
    "    if filter_ys != None:# If there's a filter     \n",
    "        dat = dat[dat['issue_year'].isin(filter_ys)]\n",
    "    \n",
    "    tempdict = {}    \n",
    "    for col in varlist:\n",
    "        tempdict[col] = {}\n",
    "    \n",
    "    for col in varlist:\n",
    "        tempdict[col]['min'] = np.nanmin(dat[col].values)\n",
    "        tempdict[col]['max'] = np.nanmax(dat[col].values)\n",
    "        tempdict[col]['95th'] = np.nanpercentile(dat[col].values, 95)\n",
    "        # TODO: Add missingness % as well\n",
    "    print(len(tempdict))\n",
    "    return tempdict\n",
    "\n",
    "#get_cont_distribution_dict = utils.get_cont_distribution_dict# TODO: Uncomment this later\n",
    "\n",
    "qavals_dict = {'continuous':{}, 'cat':{}}\n",
    "qavals_dict['continuous'] = get_cont_distribution_dict(dat, varlist = numeric_cols_4_model, filter_ys = [2015])\n",
    "\n",
    "# Now output\n",
    "pickle.dump(qavals_dict, open(FPATH + QAFNAME, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 3 digit zipcode with high risks (for dummy_creation later)\n",
    "ToDo 01/09/20: Might group some zips together to make more than one var\n",
    "Or might create one var only that has levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['938', '643', '682', '502', '692', '345', '503', '522', '528', '555', '709', '742', '901', '007', '835', '204', '569', '969', '203', '738']\n"
     ]
    }
   ],
   "source": [
    "def find_highrisk_zips(dat):\n",
    "    dat['zip3digit'] = dat.zip_code.apply(lambda z: z[:3])\n",
    "\n",
    "    # convert zip3digit variable to string\n",
    "    dat['zip3digit'] = dat.zip3digit.astype(str)\n",
    "\n",
    "    # Got top zipcodes that have largest average default rate\n",
    "    zipcode_default = dat.groupby('zip3digit')['Default_flag'].sum()\n",
    "    zipcode_all = dat.groupby('zip3digit').size()\n",
    "    zipcode_default_ratio = zipcode_default/zipcode_all\n",
    "    zipcode_default_ratio = zipcode_default_ratio.sort_values(ascending=False)\n",
    "\n",
    "    # Get numer of zipcode to collect using a default threshold\n",
    "    topn = (zipcode_default_ratio > 0.3).sum()\n",
    "\n",
    "    # Get the list of high risk zipcode\n",
    "    highr_zips = zipcode_default_ratio[:topn].index.tolist()\n",
    "    return highr_zips\n",
    "\n",
    "highrisk_zips = find_highrisk_zips(dat)\n",
    "print(highrisk_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some special variables to notice: replace NA by 0 would be wrong `mths_since_last_major_derog`, `mths_since_recent_bc_dlq`, `mths_since_recent_revol_delinq` ==> have dummy variables to mark those NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_pandas(df, cat_colname):\n",
    "    '''\n",
    "    Create one-hot-encode columns for categorical variables\n",
    "   \n",
    "    Input: a dataframe with name of column that has categorical data\n",
    "    Output: a dataframe with number of columns equivalent to number of categories\n",
    "    '''\n",
    "    df[cat_colname] = pd.Categorical(df[cat_colname])\n",
    "    dfDummies = pd.get_dummies(df[cat_colname], prefix = cat_colname)\n",
    "   \n",
    "    return dfDummies\n",
    " \n",
    "def process_cat_vars(df, cat_vars, highRiskZip):\n",
    "    \"\"\"\n",
    "    - Change values to be convenient (match with new data, lowering cases etc.)\n",
    "    - Then create features\"\"\"\n",
    "    \n",
    "    # Return all cat_names created here\n",
    "    cat_names_list = []\n",
    "    \n",
    "    # Get 3 digit zipcode\n",
    "    df['zip3digit'] = df.zip_code.apply(lambda z: z[:3])\n",
    "    # Convert zip3digit to str:\n",
    "    df['zip3digit'] = df.zip3digit.astype(str)\n",
    "    # Create a column to flag people in high risk location\n",
    "    df['highRiskZip'] = df['zip3digit'].apply(lambda x: 1 if x in highRiskZip else 0)\n",
    "    \n",
    "    # add name of new column to created_cat_names\n",
    "    cat_names_list.append('highRiskZip')\n",
    "    # Create dummy variable to flag month_since_$ variables\n",
    "    age_vars = ['mths_since_last_major_derog','mths_since_recent_bc_dlq','mths_since_recent_revol_delinq']\n",
    "\n",
    "    dummy_vars = ['major_derog_NA','bc_dlq_NA','revol_delinq_NA']\n",
    "\n",
    "    for a,d in zip(age_vars, dummy_vars):\n",
    "        df[d] = df[a].isnull().astype(int)\n",
    "        cat_names_list.append(d)\n",
    "        \n",
    "    # Clean up some cat vars\n",
    "    df['term'] = df['term'].apply(lambda x: '36' if '36' in str(x) else '60')\n",
    "    df['grade']=df['grade'].apply(lambda x: str(x).lower())\n",
    "    df['emp_length'] = df['emp_length'].apply(lambda x: str(x).lower())\n",
    "    df['home_ownership']=df['home_ownership'].apply(lambda x: str(x).lower())\n",
    "    df['purpose'] =df['purpose'].apply(lambda x: re.sub('[^a-z]+',' ', str(x).lower()) )\n",
    "    df['disbursement_method'] = df['disbursement_method'].apply(lambda x: str(x).lower())\n",
    "    df['application_type'] =df['application_type'].apply(lambda x: str(x).lower())\n",
    "    df['verification_status'] = df['verification_status'].apply(lambda x: str(x).lower())\n",
    "    \n",
    "    # One-hot-encode categorical variables\n",
    "    dfCats = pd.DataFrame()\n",
    "\n",
    "    for col in cat_vars:\n",
    "        dfDummies = one_hot_encode_pandas(df, col)\n",
    "\n",
    "        assert dfDummies.shape[0] == df.shape[0]\n",
    "\n",
    "        dfCats = pd.concat([dfCats, dfDummies], axis=1)\n",
    "\n",
    "    # Rename one column so XGBoost can fit the model\n",
    "    dfCats.rename(columns = {'emp_length_< 1 year': 'emp_length_less 1 year'}, inplace = True)\n",
    "    \n",
    "    # Add new column names to list\n",
    "    cat_names_list.extend(dfCats.columns)    \n",
    "    df = pd.concat([df, dfCats], axis = 1)\n",
    "    \n",
    "    return df[['id'] + CAT_NAMES_TO_KEEP], cat_names_list\n",
    "\n",
    "def feature_engineer_numeric_vars(df, numeric_vars):\n",
    "    \"\"\"This takes numeric vars and process them (fill missings) then outputs that list with features only\"\"\"\n",
    "    return df[['id'] + numeric_vars].fillna(0)# Note btw that id doesn't have missing so it won't be filled w any 0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1023205, 76)\n",
      "(1023205, 46)\n",
      "(1023205, 3)\n"
     ]
    }
   ],
   "source": [
    "numeric_postfe_df = feature_engineer_numeric_vars(dat, numeric_cols_4_model)\n",
    "print(numeric_postfe_df.shape)\n",
    "\n",
    "cat_postfe_df, all_cat_vars_created = process_cat_vars(dat, CAT_VARS, highrisk_zips)\n",
    "print(cat_postfe_df.shape)\n",
    "\n",
    "other_vars_postfe_df= dat[['id', 'Default_flag', 'issue_ym']]\n",
    "print(other_vars_postfe_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = FPATH + OFNAME\n",
    "\n",
    "pd.merge(pd.merge(other_vars_postfe_df, cat_postfe_df, on = ['id']), numeric_postfe_df, on = ['id']).to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
